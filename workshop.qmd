---
jupyter: python3
---

# üöÄ LLMOps for Production RAG

<a target="_blank" href="https://colab.research.google.com/github/unionai-oss/llmops-production-rag/blob/main/workshop.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

Welcome to the LLMOps for Production RAG workshop! In this workshop, we will cover:

1. Creating a baseline RAG pipeline
2. Bootstrapping an evaluation dataset
3. RAG Hyperparameter Optimization

## üì¶ Install Dependencies

```{python}
%pip install gradio

try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    !git clone https://github.com/unionai-oss/llmops-production-rag.git
    %cd llmops-production-rag
    %pip install -r requirements.lock.txt
```

While dependencies are being installed, create an account on Union Serverless:

üëâ https://signup.union.ai/

Go to the Union Serverless dashboard to make sure you can check out the UI:

üëâ https://serverless.union.ai/

Then, login to Union on this notebook session:

```{python}
%cd /content/llmops-production-rag
!union create login --auth device-flow --serverless
```

## üîë Create OpenAI API Key Secret on Union

First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.

Then, run the following command to make the secret accessible on Union:

```{python}
!union create secret openai_api_key
```

```{python}
!union get secret
```

If you have issues with the secret, you can delete it by uncommenting the code cell below:

```{python}
#!union delete secret openai_api_key
```


## üóÇÔ∏è Creating a Baseline RAG Pipeline

Create the vector store:

```{python}
!union run --remote llmops_rag/vector_store.py create_vector_store --limit 10
```

‚ö†Ô∏è Note: The above command will take a few minutes to complete since we're building our container for the first time.

Then run the simple rag pipeline

```{python}
!union run --remote llmops_rag/rag_basic.py rag_basic --questions '["How do I read and write a pandas dataframe to csv format?"]'
```

You can also run the pipeline with an Ollama server:

```{python}
!union run --remote llmops_rag/rag_basic.py rag_basic_ollama --questions '["How do I read and write a pandas dataframe to csv format?"]'
```

### ‚ú® Maintaining a Fresh Vector Store

Let's use launch plan schedules to maintain a fresh vector store.

```{python}
!union register llmops_rag/vector_store.py
!union launchplan create_vector_store_lp --activate
```

Go to the Serverless dashboard to see the schedule in action. Then you can deactivate it with:

```{python}
!union launchplan create_vector_store_lp --deactivate
```

### üíª Run RAG pipeline with Gradio App

```{python}
import gradio as gr
from app import bot, add_message
with gr.Blocks() as demo:
    chatbot = gr.Chatbot(elem_id="chatbot", bubble_full_width=False, type="messages")

    chat_input = gr.Textbox(
        interactive=True,
        placeholder="How do I write a dataframe to csv?",
        show_label=False,
    )
    chat_msg = chat_input.submit(
        add_message, [chatbot, chat_input], [chatbot, chat_input]
    )
    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name="bot_response")
    bot_msg.then(lambda: gr.Textbox(interactive=True), None, [chat_input])


demo.launch(debug=True)
```


## ü•æ Bootstrapping an Evaluation Dataset

Then generate a question and answer dataset. This will use the raw knowledge base we created
in the previous step.

```{python}
!union run --remote llmops_rag/create_qa_dataset.py create_qa_dataset --n_questions_per_doc 5 --n_answers_per_question 5
```

Filter the dataset with an LLM critic:

```{python}
!union run --remote llmops_rag/create_llm_filtered_dataset.py create_llm_filtered_dataset
```


## üìä RAG Hyperparameter Optimization

Experiment with different embedding models:

```{python}
!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/embedding_model_experiment.yaml
```

### üß™ More experiments to run

Experiment with different chunksizes:

```{python}
!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/chunksize_experiment.yaml
```

Experiment with different splitters:

```{python}
!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/splitter_experiment.yaml
```

Experiment with reranking:

```{python}
!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/reranking_experiment.yaml
```

Experiment with document retrieval:

```{python}
!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/search_params_experiment.yaml
```

## üîÑ Putting it all together into a reactive pipeline

Register and activate the reactive pipeline:

```{python}
!union register llmops_rag/reactive_pipeline.py
!union launchplan knowledge_base_lp --activate
!union launchplan create_eval_dataset_lp --activate
!union launchplan optimize_rag_lp --activate
```

Deactivate the reactive pipeline:

```{python}
!union launchplan knowledge_base_lp --deactivate
!union launchplan create_eval_dataset_lp --deactivate
!union launchplan optimize_rag_lp --deactivate
```
