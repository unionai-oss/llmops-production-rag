{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 LLMOps for Production RAG\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/llmops-production-rag/blob/main/workshop.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to the LLMOps for Production RAG workshop! In this workshop, we will cover:\n",
    "\n",
    "1. Creating a baseline RAG pipeline\n",
    "2. Bootstrapping an evaluation dataset\n",
    "3. RAG Hyperparameter Optimization\n",
    "\n",
    "## 📦 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/llmops-production-rag.git\n",
    "    %cd llmops-production-rag\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While dependencies are being installed, create an account on Union Serverless:\n",
    "\n",
    "👉 https://signup.union.ai/\n",
    "\n",
    "Go to the Union Serverless dashboard to make sure you can check out the UI:\n",
    "\n",
    "👉 https://serverless.union.ai/\n",
    "\n",
    "Then, login to Union on this notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 \u001b[33mConfiguration saved to \u001b[0m\u001b[33m/Users/nielsbantilan/.union/\u001b[0m\u001b[33mconfig.yaml\u001b[0m\n",
      "Login successful into \u001b[1;32mserverless\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union create login --auth device-flow --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 Create OpenAI API Key Secret on Union\n",
    "\n",
    "First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.\n",
    "\n",
    "Then, run the following command to make the secret accessible on Union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create secret openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union get secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues with the secret, you can delete it by uncommenting the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!union delete secret openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ Creating a Baseline RAG Pipeline\n",
    "\n",
    "Create the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import union\n",
    "from typing import Optional\n",
    "from llmops_rag.vector_store import create_knowledge_base, chunk_and_embed_documents\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def create_vector_store(\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    splitter: str = \"character\",\n",
    "    chunk_size: int = 2048,\n",
    "    limit: Optional[int | float] = None,\n",
    "    embedding_model: Optional[str] = \"text-embedding-ada-002\",\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    ") -> union.FlyteDirectory:\n",
    "    \"\"\"\n",
    "    Workflow for creating the vector store knowledge base.\n",
    "    \"\"\"\n",
    "    docs = create_knowledge_base(\n",
    "        root_url_tags_mapping=root_url_tags_mapping,\n",
    "        limit=limit,\n",
    "        exclude_patterns=exclude_patterns,\n",
    "    )\n",
    "    vector_store = chunk_and_embed_documents(\n",
    "        documents=docs,\n",
    "        splitter=splitter,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=embedding_model,\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this workflow, let's create a `UnionRemote` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = union.UnionRemote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we execute the `create_vector_store` workflow with the `.execute()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_execution = remote.execute(\n",
    "    create_vector_store,\n",
    "    inputs=dict(\n",
    "        limit=10,\n",
    "        chunk_size=512,\n",
    "        splitter=\"character\",\n",
    "    ),\n",
    ")\n",
    "vector_store_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Note: The above command will take a few minutes to complete since we're building our container for the first time.\n",
    "\n",
    "Then implement a basic RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import union\n",
    "from typing import Optional\n",
    "from llmops_rag.vector_store import VectorStore\n",
    "from llmops_rag.rag_basic import retrieve, generate\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def rag_basic(\n",
    "    questions: list[str],\n",
    "    vector_store: union.FlyteDirectory = VectorStore.query(),  # 👈 this uses the vector store artifact by default\n",
    "    embedding_model: str = \"text-embedding-ada-002\",\n",
    "    generation_model: str = \"gpt-4o-mini\",\n",
    "    search_type: str = \"similarity\",\n",
    "    rerank: bool = False,\n",
    "    num_retrieved_docs: int = 20,\n",
    "    num_docs_final: int = 5,\n",
    "    prompt_template: Optional[str] = None,\n",
    ") -> list[str]:\n",
    "    contexts = retrieve(\n",
    "        questions=questions,\n",
    "        vector_store=vector_store,\n",
    "        embedding_model=embedding_model,\n",
    "        search_type=search_type,\n",
    "        rerank=rerank,\n",
    "        num_retrieved_docs=num_retrieved_docs,\n",
    "        num_docs_final=num_docs_final,\n",
    "    )\n",
    "    return generate(\n",
    "        questions=questions,\n",
    "        contexts=contexts,\n",
    "        generation_model=generation_model,\n",
    "        prompt_template=prompt_template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Execution is in-progress. </b> <a href='https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/a4bprqwfdjlmjnx84wpp'>https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/a4bprqwfdjlmjnx84wpp</a>"
      ],
      "text/plain": [
       "Flyte Serialized object (FlyteWorkflowExecution):\n",
       "  id:\n",
       "    project: default\n",
       "    domain: development\n",
       "    name: a4bprqwfdjlmjnx84wpp\n",
       "  spec:\n",
       "    launch_plan:\n",
       "      resource_type: 3\n",
       "      project: default\n",
       "      domain: development\n",
       "      name: rag_basic\n",
       "      version: JKIX0CPY4xPAD1JPuhiYFA\n",
       "    metadata:\n",
       "      principal: 00ug8tl4tit8Dvpi65d7\n",
       "  closure:\n",
       "    created_at:\n",
       "      seconds: 1749746581\n",
       "      nanos: 444031000\n",
       "    updated_at:\n",
       "      seconds: 1749746581\n",
       "      nanos: 444031000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote = union.UnionRemote()\n",
    "rag_basic_execution = remote.execute(\n",
    "    rag_basic,\n",
    "    inputs=dict(\n",
    "        questions=[\"How do I read and write a pandas dataframe to csv format?\"],\n",
    "    ),\n",
    ")\n",
    "rag_basic_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the output of the rag pipeline execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To read and write a pandas DataFrame to CSV format, you can use the `to_csv()` method for writing and the `read_csv()` function for reading.\n",
      "\n",
      "### Writing a DataFrame to CSV\n",
      "\n",
      "To write a DataFrame to a CSV file, you need to call the `to_csv()` method on your DataFrame object. You can specify the filename as a string argument. Here's an example:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(np.random.randint(0, 5, (10, 5)))\n",
      "\n",
      "# Write the DataFrame to a CSV file\n",
      "df.to_csv(\"foo.csv\", index=False)  # Setting index=False to avoid writing row indices\n",
      "```\n",
      "\n",
      "In this example, a CSV file named \"foo.csv\" will be created in the current working directory. The `index=False` parameter prevents pandas from writing row indices to the file, which is often desirable unless you need them.\n",
      "\n",
      "### Reading a CSV file into a DataFrame\n",
      "\n",
      "To read data from a CSV file back into a DataFrame, you can use the `read_csv()` function. You simply provide the filename of the CSV file as an argument. Here's how to do it:\n",
      "\n",
      "```python\n",
      "# Read the CSV file into a DataFrame\n",
      "df_read = pd.read_csv(\"foo.csv\")\n",
      "\n",
      "# Display the DataFrame\n",
      "print(df_read)\n",
      "```\n",
      "\n",
      "This code will read the contents of \"foo.csv\" into a new DataFrame called `df_read`. The DataFrame now contains the data previously saved in the CSV file.\n",
      "\n",
      "### Additional Options\n",
      "\n",
      "Both `to_csv()` and `read_csv()` provide various parameters for customized behavior, such as specifying the delimiter, encoding, handling missing values, and more. You can check the pandas documentation for these methods to explore all available options.\n"
     ]
    }
   ],
   "source": [
    "rag_basic_execution = remote.sync(remote.wait(rag_basic_execution))\n",
    "print(rag_basic_execution.outputs[\"o0\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✨ Maintaining a Fresh Vector Store\n",
    "\n",
    "Let's use launch plan schedules to maintain a fresh vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import flytekit as fl\n",
    "\n",
    "\n",
    "union.LaunchPlan.CACHE = {}\n",
    "schedule_vector_store_lp = union.LaunchPlan.get_or_create(\n",
    "    name=\"schedule_vector_store_lp\",\n",
    "    workflow=create_vector_store,\n",
    "    default_inputs=dict(\n",
    "        limit=10,\n",
    "        chunk_size=512,\n",
    "        splitter=\"character\",\n",
    "    ),\n",
    "    schedule=fl.FixedRate(\n",
    "        duration=timedelta(minutes=2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mImage llmops-rag:2Md8n7sPXwjAbEs6KJjTqg was not found or has expired.\u001b[0m\n",
      "\u001b[34m\u001b[1m🐳 Submitting a new build...\u001b[0m\n",
      "\u001b[33m\u001b[1m👍 Build submitted!\u001b[0m\n",
      "\u001b[1m⏳ Waiting for build to finish at: \u001b[36mhttps://serverless.union.ai/org/cosmicbboy/projects/system/domains/production/executions/avzspwnwzm7rr6bmrktq\u001b[0m\u001b[0m\n",
      "\u001b[32m\u001b[1m✅ Build completed in 0:01:07!\u001b[0m\n",
      "🚀 Launch plan activated\n"
     ]
    }
   ],
   "source": [
    "version = \"test-v10\"\n",
    "remote = union.UnionRemote()\n",
    "# remote.register_workflow(create_vector_store, version=version)\n",
    "registered_schedule_vector_store_lp = remote.register_launch_plan(\n",
    "    schedule_vector_store_lp,\n",
    "    version=version,\n",
    ")\n",
    "remote.activate_launchplan(registered_schedule_vector_store_lp.id)\n",
    "print(\"🚀 Launch plan activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote.deactivate_launchplan(registered_schedule_vector_store_lp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Serverless dashboard to see the schedule in action.\n",
    "\n",
    "Make sure to deactivate the launchplan in the UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 Run RAG pipeline with Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://d890e65b965b5d3fe4.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d890e65b965b5d3fe4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Union Serverless execution url: https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/a4mgvxb6kwqjlpwvpg74\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://d890e65b965b5d3fe4.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append({\"role\": \"user\", \"content\": message})\n",
    "    return history, gr.Textbox(value=None, interactive=False)\n",
    "\n",
    "\n",
    "def bot(history: list):\n",
    "    remote = union.UnionRemote()\n",
    "    last_user_message = [msg for msg in history if msg[\"role\"] == \"user\"][-1][\"content\"]\n",
    "    execution = remote.execute(rag_basic, inputs={\"questions\": [last_user_message]})\n",
    "    url = remote.generate_console_url(execution)\n",
    "    print(f\"🚀 Union Serverless execution url: {url}\")\n",
    "\n",
    "    answers = None\n",
    "    execution = remote.wait(execution, poll_interval=timedelta(seconds=2))\n",
    "    answers = execution.outputs[\"o0\"]\n",
    "\n",
    "    if answers is None:\n",
    "        raise RuntimeError(\"Failed to get answer\")\n",
    "    \n",
    "    answer = answers[0]\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    history[-1][\"content\"] += answer\n",
    "    yield history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", type=\"messages\")\n",
    "\n",
    "    chat_input = gr.Textbox(\n",
    "        interactive=True,\n",
    "        placeholder=\"How do I write a dataframe to csv?\",\n",
    "        show_label=False,\n",
    "    )\n",
    "    chat_msg = chat_input.submit(\n",
    "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "    )\n",
    "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.Textbox(interactive=True), None, [chat_input])\n",
    "\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥾 Bootstrapping an Evaluation Dataset\n",
    "\n",
    "Then generate a question and answer dataset. This will use the raw knowledge base we created\n",
    "in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Annotated\n",
    "\n",
    "import union\n",
    "from llmops_rag.create_qa_dataset import generate_qa_datapoints, create_dataset, QuestionAndAnswerDataset\n",
    "from llmops_rag.document import CustomDocument\n",
    "from llmops_rag.vector_store import KnowledgeBase\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def create_qa_dataset(\n",
    "    documents: list[CustomDocument] = KnowledgeBase.query(),\n",
    "    n_questions_per_doc: int = 1,\n",
    "    n_answers_per_question: int = 5,\n",
    ") -> Annotated[union.FlyteFile, QuestionAndAnswerDataset]:\n",
    "    partial_task = functools.partial(\n",
    "        generate_qa_datapoints,\n",
    "        n_questions_per_doc=n_questions_per_doc,\n",
    "        n_answers_per_question=n_answers_per_question,\n",
    "    )\n",
    "    questions_and_answers = union.map_task(partial_task)(flyte_doc=documents)\n",
    "    return create_dataset(questions_and_answers, n_answers_per_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Execution is in-progress. </b> <a href='https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/aswns6rm6xmswzb8w54h'>https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/aswns6rm6xmswzb8w54h</a>"
      ],
      "text/plain": [
       "Flyte Serialized object (FlyteWorkflowExecution):\n",
       "  id:\n",
       "    project: default\n",
       "    domain: development\n",
       "    name: aswns6rm6xmswzb8w54h\n",
       "  spec:\n",
       "    launch_plan:\n",
       "      resource_type: 3\n",
       "      project: default\n",
       "      domain: development\n",
       "      name: create_qa_dataset\n",
       "      version: hsepqO7DKz4x_Pm2J63hag\n",
       "    metadata:\n",
       "      principal: 00ug8tl4tit8Dvpi65d7\n",
       "  closure:\n",
       "    created_at:\n",
       "      seconds: 1749746896\n",
       "      nanos: 848822000\n",
       "    updated_at:\n",
       "      seconds: 1749746896\n",
       "      nanos: 848822000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_execution = remote.execute(\n",
    "    create_qa_dataset,\n",
    "    inputs={\"n_questions_per_doc\": 3, \"n_answers_per_question\": 5}\n",
    ")\n",
    "qa_dataset_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset with an LLM critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import union\n",
    "from llmops_rag.create_llm_filtered_dataset import apply_llm_critic, filter_dataset, prepare_dataset\n",
    "from llmops_rag.create_qa_dataset import QuestionAndAnswerDataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def create_llm_filtered_dataset(\n",
    "    dataset: union.FlyteFile = QuestionAndAnswerDataset.query(),\n",
    ") -> pd.DataFrame:\n",
    "    scores = apply_llm_critic(dataset)\n",
    "    reference_answers = filter_dataset(dataset, scores)\n",
    "    return prepare_dataset(reference_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Execution is in-progress. </b> <a href='https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/a48lb6p6m2vz2bvcftjb'>https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/a48lb6p6m2vz2bvcftjb</a>"
      ],
      "text/plain": [
       "Flyte Serialized object (FlyteWorkflowExecution):\n",
       "  id:\n",
       "    project: default\n",
       "    domain: development\n",
       "    name: a48lb6p6m2vz2bvcftjb\n",
       "  spec:\n",
       "    launch_plan:\n",
       "      resource_type: 3\n",
       "      project: default\n",
       "      domain: development\n",
       "      name: create_llm_filtered_dataset\n",
       "      version: QAlGm0Ldv5EPlJEyBEB5MA\n",
       "    metadata:\n",
       "      principal: 00ug8tl4tit8Dvpi65d7\n",
       "  closure:\n",
       "    created_at:\n",
       "      seconds: 1749747244\n",
       "      nanos: 442508000\n",
       "    updated_at:\n",
       "      seconds: 1749747244\n",
       "      nanos: 442508000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset_execution = remote.execute(create_llm_filtered_dataset, inputs={})\n",
    "filtered_dataset_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 RAG Hyperparameter Optimization\n",
    "\n",
    "Experiment with different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import union\n",
    "from typing import Optional, Annotated\n",
    "from llmops_rag.optimize_rag import (\n",
    "    GridSearchConfig,\n",
    "    prepare_hpo_configs,\n",
    "    prepare_questions,\n",
    "    gridsearch,\n",
    "    combine_answers,\n",
    "    evaluate,\n",
    "    report,\n",
    ")\n",
    "from llmops_rag.config import RAGConfig\n",
    "from llmops_rag.create_llm_filtered_dataset import EvalDatasetArtifact\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def optimize_rag(\n",
    "    gridsearch_config: GridSearchConfig,\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    "    limit: Optional[int] = 10,\n",
    "    eval_dataset: Annotated[pd.DataFrame, EvalDatasetArtifact] = EvalDatasetArtifact.query(dataset_type=\"llm_filtered\"),\n",
    "    eval_prompt_template: Optional[str] = None,\n",
    "    n_answers: int = 1,\n",
    ") -> RAGConfig:\n",
    "    hpo_configs = prepare_hpo_configs(gridsearch_config)\n",
    "    questions = prepare_questions(eval_dataset, n_answers)\n",
    "    answers = gridsearch(questions, hpo_configs, root_url_tags_mapping, exclude_patterns, limit)\n",
    "    answers_dataset = combine_answers(answers, hpo_configs, questions)\n",
    "    best_config, evaluation, evalution_summary = evaluate(\n",
    "        answers_dataset, eval_prompt_template\n",
    "    )\n",
    "    report(evaluation, evalution_summary)\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Execution is in-progress. </b> <a href='https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/ax9f56dk4n252wvpkmm4'>https://serverless.union.ai/org/cosmicbboy/projects/default/domains/development/executions/ax9f56dk4n252wvpkmm4</a>"
      ],
      "text/plain": [
       "Flyte Serialized object (FlyteWorkflowExecution):\n",
       "  id:\n",
       "    project: default\n",
       "    domain: development\n",
       "    name: ax9f56dk4n252wvpkmm4\n",
       "  spec:\n",
       "    launch_plan:\n",
       "      resource_type: 3\n",
       "      project: default\n",
       "      domain: development\n",
       "      name: optimize_rag\n",
       "      version: jQ9Af1vEhbcLSgGgXucSOA\n",
       "    metadata:\n",
       "      principal: 00ug8tl4tit8Dvpi65d7\n",
       "  closure:\n",
       "    created_at:\n",
       "      seconds: 1749747733\n",
       "      nanos: 738816000\n",
       "    updated_at:\n",
       "      seconds: 1749747733\n",
       "      nanos: 738816000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def run_experiment(config_path: str):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        gridsearch_config = GridSearchConfig(**yaml.safe_load(f))\n",
    "\n",
    "    remote = union.UnionRemote()\n",
    "    execution = remote.execute(optimize_rag, inputs={\"gridsearch_config\": gridsearch_config})\n",
    "    return execution\n",
    "\n",
    "\n",
    "run_experiment(\"config/embedding_model_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 More experiments to run (optional)\n",
    "\n",
    "Uncomment the code cells below to run different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different chunksizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(\"config/chunksize_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different splitters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(\"config/splitter_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with reranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(\"config/reranking_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with document retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(\"config/search_params_experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Putting it all together into a reactive pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Annotated\n",
    "from datetime import timedelta\n",
    "\n",
    "import union\n",
    "import flytekit as fl\n",
    "import pandas as pd\n",
    "from union.artifacts import OnArtifact\n",
    "\n",
    "from llmops_rag.document import CustomDocument\n",
    "from llmops_rag.create_qa_dataset import create_qa_dataset\n",
    "from llmops_rag.create_llm_filtered_dataset import create_llm_filtered_dataset, EvalDatasetArtifact\n",
    "from llmops_rag.image import image\n",
    "from llmops_rag.optimize_rag import optimize_rag, GridSearchConfig\n",
    "from llmops_rag.vector_store import create_knowledge_base as _create_knowledge_base\n",
    "\n",
    "\n",
    "# Clear the launch plan cache\n",
    "union.LaunchPlan.CACHE = {}\n",
    "\n",
    "\n",
    "KnowledgeBaseHPO = union.Artifact(name=\"knowledge-base-hpo\")\n",
    "\n",
    "\n",
    "@union.task(\n",
    "    container_image=image,\n",
    "    requests=union.Resources(cpu=\"2\", mem=\"8Gi\"),\n",
    "    enable_deck=True,\n",
    ")\n",
    "def create_knowledge_base_hpo(\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    limit: Optional[int | float] = None,\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    ") -> Annotated[list[CustomDocument], KnowledgeBaseHPO]:\n",
    "    return _create_knowledge_base.task_function(\n",
    "        root_url_tags_mapping=root_url_tags_mapping,\n",
    "        limit=limit,\n",
    "        exclude_patterns=exclude_patterns,\n",
    "    )\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def knowledge_base_workflow(\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    limit: Optional[int | float] = None,\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    ") -> list[CustomDocument]:\n",
    "    return create_knowledge_base_hpo(\n",
    "        root_url_tags_mapping=root_url_tags_mapping,\n",
    "        limit=limit,\n",
    "        exclude_patterns=exclude_patterns,\n",
    "    )\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def create_eval_dataset(\n",
    "    documents: list[CustomDocument],\n",
    "    n_questions_per_doc: int = 1,\n",
    "    n_answers_per_question: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    qa_dataset = create_qa_dataset(\n",
    "        documents=documents,\n",
    "        n_questions_per_doc=n_questions_per_doc,\n",
    "        n_answers_per_question=n_answers_per_question,\n",
    "    )\n",
    "    return create_llm_filtered_dataset(dataset=qa_dataset)\n",
    "\n",
    "\n",
    "knowledge_base_lp = union.LaunchPlan.get_or_create(\n",
    "    knowledge_base_workflow,\n",
    "    name=\"knowledge_base_lp\",\n",
    "    default_inputs={\"limit\": 10},\n",
    "    schedule=fl.FixedRate(duration=timedelta(minutes=2))\n",
    ")\n",
    "\n",
    "create_eval_dataset_lp = union.LaunchPlan.get_or_create(\n",
    "    create_eval_dataset,\n",
    "    name=\"create_eval_dataset_lp\",\n",
    "    trigger=OnArtifact(\n",
    "        trigger_on=KnowledgeBaseHPO,\n",
    "        inputs={\"documents\": KnowledgeBaseHPO.query()},\n",
    "    )\n",
    ")\n",
    "\n",
    "optimize_rag_lp = union.LaunchPlan.get_or_create(\n",
    "    optimize_rag,\n",
    "    name=\"optimize_rag_lp\",\n",
    "    default_inputs={\n",
    "        \"gridsearch_config\": GridSearchConfig(\n",
    "            embedding_model=[\n",
    "                \"text-embedding-ada-002\",\n",
    "                \"text-embedding-3-small\",\n",
    "                \"text-embedding-3-large\",\n",
    "            ],\n",
    "            chunk_size=[256],\n",
    "            splitter=[\"recursive\"],\n",
    "        ),\n",
    "    },\n",
    "    trigger=OnArtifact(\n",
    "        trigger_on=EvalDatasetArtifact,\n",
    "        inputs={\"eval_dataset\": EvalDatasetArtifact.query()},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mImage llmops-rag:2Md8n7sPXwjAbEs6KJjTqg found. Skip building.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "remote = union.UnionRemote()\n",
    "\n",
    "version = \"v0\"\n",
    "registered_lps = []\n",
    "for lp in [\n",
    "    knowledge_base_lp,\n",
    "    create_eval_dataset_lp,\n",
    "    optimize_rag_lp,\n",
    "]:\n",
    "    registered_lp = remote.register_launch_plan(lp, version=version)\n",
    "    registered_lps.append(registered_lp)\n",
    "    remote.activate_launchplan(registered_lp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deactivate the reactive pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for registered_lp in registered_lps:\n",
    "    remote.deactivate_launchplan(registered_lp.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
