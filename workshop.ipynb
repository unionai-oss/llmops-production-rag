{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LLMOps for Production RAG\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/llmops-production-rag/blob/main/workshop.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to the LLMOps for Production RAG workshop! In this workshop, we will cover:\n",
    "\n",
    "1. Creating a baseline RAG pipeline\n",
    "2. Bootstrapping an evaluation dataset\n",
    "3. RAG Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/llmops-production-rag.git\n",
    "    %cd llmops-production-rag\n",
    "    %pip install -r requirements.lock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/llmops-production-rag\n",
    "!union create login --auth device-flow --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Create OpenAI API Key Secret on Union\n",
    "\n",
    "First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.\n",
    "\n",
    "Then, run the following command to make the secret accessible on Union:\n",
    "\n",
    "```\n",
    "!union create secret openai_api_key\n",
    "```\n",
    "\n",
    "```\n",
    "!union get secret\n",
    "```\n",
    "\n",
    "If you have issues with the secret, you can delete it by uncommenting the code cell below:\n",
    "\n",
    "```\n",
    "#!union delete secret openai_api_key\n",
    "```\n",
    "\n",
    "\n",
    "## üóÇÔ∏è Creating a Baseline RAG Pipeline\n",
    "\n",
    "Create the vector store:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/vector_store.py create_vector_store --limit 10\n",
    "```\n",
    "\n",
    "Then run the simple rag pipeline\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/rag_basic.py rag_basic --questions '[\"How do I read and write a pandas dataframe to csv format?\"]'\n",
    "```\n",
    "\n",
    "You can also run the pipeline with an Ollama server:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/rag_basic.py rag_basic_ollama --questions '[\"How do I read and write a pandas dataframe to csv format?\"]'\n",
    "```\n",
    "\n",
    "### üíª Run RAG pipeline with Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from app import bot, add_message\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", bubble_full_width=False, type=\"messages\")\n",
    "\n",
    "    chat_input = gr.Textbox(\n",
    "        interactive=True,\n",
    "        placeholder=\"How do I write a dataframe to csv?\",\n",
    "        show_label=False,\n",
    "    )\n",
    "    chat_msg = chat_input.submit(\n",
    "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "    )\n",
    "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.Textbox(interactive=True), None, [chat_input])\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•æ Bootstrapping an Evaluation Dataset\n",
    "\n",
    "Then generate a question and answer dataset. This will use the raw knowledge base we created\n",
    "in the previous step.\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/create_qa_dataset.py create_qa_dataset --n_questions_per_doc 5 --n_answers_per_question 5\n",
    "```\n",
    "\n",
    "Filter the dataset with an LLM critic:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/create_llm_filtered_dataset.py create_llm_filtered_dataset\n",
    "```\n",
    "\n",
    "\n",
    "## üìä RAG Hyperparameter Optimization\n",
    "\n",
    "Experiment with different chunksizes:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/chunksize_experiment.yaml\n",
    "```\n",
    "\n",
    "Experiment with different embedding models:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/embedding_model_experiment.yaml\n",
    "```\n",
    "\n",
    "## üß™ More experiments to run\n",
    "\n",
    "Experiment with different splitters:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/splitter_experiment.yaml\n",
    "```\n",
    "\n",
    "Experiment with reranking:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/reranking_experiment.yaml\n",
    "```\n",
    "\n",
    "Experiment with document retrieval:\n",
    "\n",
    "```\n",
    "!union run --remote llmops_rag/optimize_rag.py optimize_rag --gridsearch_config config/search_params_experiment.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
