{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ LLMOps for Production RAG\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/llmops-production-rag/blob/main/workshop.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to the LLMOps for Production RAG workshop! In this workshop, we will cover:\n",
    "\n",
    "1. Creating a baseline RAG pipeline\n",
    "2. Bootstrapping an evaluation dataset\n",
    "3. RAG Hyperparameter Optimization\n",
    "\n",
    "## ðŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/llmops-production-rag.git\n",
    "    %cd llmops-production-rag\n",
    "    %pip install -r requirements.lock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While dependencies are being installed, create an account on Union Serverless:\n",
    "\n",
    "ðŸ‘‰ https://signup.union.ai/\n",
    "\n",
    "Go to the Union Serverless dashboard to make sure you can check out the UI:\n",
    "\n",
    "ðŸ‘‰ https://serverless.union.ai/\n",
    "\n",
    "Then, login to Union on this notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/llmops-production-rag\n",
    "!union create login --auth device-flow --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Create OpenAI API Key Secret on Union\n",
    "\n",
    "First go to https://platform.openai.com/account/api-keys and create an OpenAI API key.\n",
    "\n",
    "Then, run the following command to make the secret accessible on Union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create secret openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union get secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues with the secret, you can delete it by uncommenting the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!union delete secret openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Creating a Baseline RAG Pipeline\n",
    "\n",
    "Create the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_store.py\n",
    "import flytekit as fl\n",
    "from typing import Optional\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from llmops_rag.vector_store import create_knowledge_base, chunk_and_embed_documents\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def create_vector_store(\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    splitter: str = \"character\",\n",
    "    chunk_size: int = 2048,\n",
    "    limit: Optional[int | float] = None,\n",
    "    embedding_model: Optional[str] = \"text-embedding-ada-002\",\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    ") -> FlyteDirectory:\n",
    "    \"\"\"\n",
    "    Workflow for creating the vector store knowledge base.\n",
    "    \"\"\"\n",
    "    docs = create_knowledge_base(\n",
    "        root_url_tags_mapping=root_url_tags_mapping,\n",
    "        limit=limit,\n",
    "        exclude_patterns=exclude_patterns,\n",
    "    )\n",
    "    vector_store = chunk_and_embed_documents(\n",
    "        documents=docs,\n",
    "        splitter=splitter,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=embedding_model,\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote vector_store.py create_vector_store --limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ Note: The above command will take a few minutes to complete since we're building our container for the first time.\n",
    "\n",
    "Then implement a basic RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_basic.py\n",
    "import flytekit as fl\n",
    "from typing import Optional\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from llmops_rag.vector_store import VectorStore\n",
    "from llmops_rag.rag_basic import retrieve, generate\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def rag_basic(\n",
    "    questions: list[str],\n",
    "    vector_store: FlyteDirectory = VectorStore.query(),  # ðŸ‘ˆ this uses the vector store artifact by default\n",
    "    embedding_model: str = \"text-embedding-ada-002\",\n",
    "    generation_model: str = \"gpt-4o-mini\",\n",
    "    search_type: str = \"similarity\",\n",
    "    rerank: bool = False,\n",
    "    num_retrieved_docs: int = 20,\n",
    "    num_docs_final: int = 5,\n",
    "    prompt_template: Optional[str] = None,\n",
    ") -> list[str]:\n",
    "    contexts = retrieve(\n",
    "        questions=questions,\n",
    "        vector_store=vector_store,\n",
    "        embedding_model=embedding_model,\n",
    "        search_type=search_type,\n",
    "        rerank=rerank,\n",
    "        num_retrieved_docs=num_retrieved_docs,\n",
    "        num_docs_final=num_docs_final,\n",
    "    )\n",
    "    return generate(\n",
    "        questions=questions,\n",
    "        contexts=contexts,\n",
    "        generation_model=generation_model,\n",
    "        prompt_template=prompt_template,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run it with `union run`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote rag_basic.py rag_basic --questions '[\"How do I read and write a pandas dataframe to csv format?\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the pipeline with an Ollama server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote llmops_rag/rag_basic.py rag_basic_ollama --questions '[\"How do I read and write a pandas dataframe to csv format?\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ¨ Maintaining a Fresh Vector Store\n",
    "\n",
    "Let's use launch plan schedules to maintain a fresh vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_store_launchplan.py\n",
    "import flytekit as fl\n",
    "from datetime import timedelta\n",
    "from vector_store import create_vector_store\n",
    "\n",
    "\n",
    "schedule_vector_store_lp = fl.LaunchPlan.get_or_create(\n",
    "    name=\"schedule_vector_store_lp\",\n",
    "    workflow=create_vector_store,\n",
    "    default_inputs={\n",
    "        \"limit\": 10,\n",
    "    },\n",
    "    schedule=fl.FixedRate(\n",
    "        duration=timedelta(minutes=3)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union register vector_store_launchplan.py\n",
    "!union launchplan schedule_vector_store_lp --activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Serverless dashboard to see the schedule in action. Then you can deactivate it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union launchplan schedule_vector_store_lp --deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’» Run RAG pipeline with Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from app import bot, add_message\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", bubble_full_width=False, type=\"messages\")\n",
    "\n",
    "    chat_input = gr.Textbox(\n",
    "        interactive=True,\n",
    "        placeholder=\"How do I write a dataframe to csv?\",\n",
    "        show_label=False,\n",
    "    )\n",
    "    chat_msg = chat_input.submit(\n",
    "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "    )\n",
    "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.Textbox(interactive=True), None, [chat_input])\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥¾ Bootstrapping an Evaluation Dataset\n",
    "\n",
    "Then generate a question and answer dataset. This will use the raw knowledge base we created\n",
    "in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_qa_dataset.py\n",
    "import functools\n",
    "from typing import Annotated\n",
    "\n",
    "import flytekit as fl\n",
    "from flytekit.types.file import FlyteFile\n",
    "from llmops_rag.create_qa_dataset import generate_qa_datapoints, create_dataset, QuestionAndAnswerDataset\n",
    "from llmops_rag.document import CustomDocument\n",
    "from llmops_rag.vector_store import KnowledgeBase\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def create_qa_dataset(\n",
    "    documents: list[CustomDocument] = KnowledgeBase.query(),\n",
    "    n_questions_per_doc: int = 1,\n",
    "    n_answers_per_question: int = 5,\n",
    ") -> Annotated[FlyteFile, QuestionAndAnswerDataset]:\n",
    "    partial_task = functools.partial(\n",
    "        generate_qa_datapoints,\n",
    "        n_questions_per_doc=n_questions_per_doc,\n",
    "        n_answers_per_question=n_answers_per_question,\n",
    "    )\n",
    "    questions_and_answers = fl.map_task(partial_task)(flyte_doc=documents)\n",
    "    return create_dataset(questions_and_answers, n_answers_per_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote create_qa_dataset.py create_qa_dataset --n_questions_per_doc 5 --n_answers_per_question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataset with an LLM critic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_llm_filtered_dataset.py\n",
    "import flytekit as fl\n",
    "from llmops_rag.create_llm_filtered_dataset import apply_llm_critic, filter_dataset, prepare_dataset\n",
    "from llmops_rag.create_qa_dataset import QuestionAndAnswerDataset\n",
    "from flytekit.types.file import FlyteFile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def create_llm_filtered_dataset(\n",
    "    dataset: FlyteFile = QuestionAndAnswerDataset.query(),\n",
    ") -> pd.DataFrame:\n",
    "    scores = apply_llm_critic(dataset)\n",
    "    reference_answers = filter_dataset(dataset, scores)\n",
    "    return prepare_dataset(reference_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote create_llm_filtered_dataset.py create_llm_filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š RAG Hyperparameter Optimization\n",
    "\n",
    "Experiment with different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile optimize_rag.py\n",
    "import flytekit as fl\n",
    "from typing import Optional, Annotated\n",
    "from llmops_rag.optimize_rag import (\n",
    "    GridSearchConfig,\n",
    "    prepare_hpo_configs,\n",
    "    prepare_questions,\n",
    "    gridsearch,\n",
    "    combine_answers,\n",
    "    evaluate,\n",
    "    report,\n",
    ")\n",
    "from llmops_rag.config import RAGConfig\n",
    "from llmops_rag.create_llm_filtered_dataset import EvalDatasetArtifact\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def optimize_rag(\n",
    "    gridsearch_config: GridSearchConfig,\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    "    limit: Optional[int] = 10,\n",
    "    eval_dataset: Annotated[pd.DataFrame, EvalDatasetArtifact] = EvalDatasetArtifact.query(dataset_type=\"llm_filtered\"),\n",
    "    eval_prompt_template: Optional[str] = None,\n",
    "    n_answers: int = 1,\n",
    ") -> RAGConfig:\n",
    "    hpo_configs = prepare_hpo_configs(gridsearch_config)\n",
    "    questions = prepare_questions(eval_dataset, n_answers)\n",
    "    answers = gridsearch(questions, hpo_configs, root_url_tags_mapping, exclude_patterns, limit)\n",
    "    answers_dataset = combine_answers(answers, hpo_configs, questions)\n",
    "    best_config, evaluation, evalution_summary = evaluate(\n",
    "        answers_dataset, eval_prompt_template\n",
    "    )\n",
    "    report(evaluation, evalution_summary)\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote optimize_rag.py optimize_rag --gridsearch_config config/embedding_model_experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª More experiments to run\n",
    "\n",
    "Experiment with different chunksizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote optimize_rag.py optimize_rag --gridsearch_config config/chunksize_experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different splitters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote optimize_rag.py optimize_rag --gridsearch_config config/splitter_experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with reranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote optimize_rag.py optimize_rag --gridsearch_config config/reranking_experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with document retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote optimize_rag.py optimize_rag --gridsearch_config config/search_params_experiment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Putting it all together into a reactive pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reactive_pipeline.py\n",
    "from typing import Annotated, Optional\n",
    "from datetime import timedelta\n",
    "\n",
    "import flytekit as fl\n",
    "import pandas as pd\n",
    "from union.artifacts import OnArtifact\n",
    "\n",
    "from llmops_rag.document import CustomDocument\n",
    "from llmops_rag.create_qa_dataset import create_qa_dataset\n",
    "from llmops_rag.create_llm_filtered_dataset import create_llm_filtered_dataset, EvalDatasetArtifact\n",
    "from llmops_rag.optimize_rag import optimize_rag, GridSearchConfig\n",
    "from llmops_rag.vector_store import create_knowledge_base, KnowledgeBase\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def knowledge_base_workflow(\n",
    "    root_url_tags_mapping: Optional[dict] = None,\n",
    "    limit: Optional[int | float] = None,\n",
    "    exclude_patterns: Optional[list[str]] = None,\n",
    ") -> list[CustomDocument]:\n",
    "    return create_knowledge_base(\n",
    "        root_url_tags_mapping=root_url_tags_mapping,\n",
    "        limit=limit,\n",
    "        exclude_patterns=exclude_patterns,\n",
    "    ).with_overrides(cache=False)\n",
    "\n",
    "\n",
    "@fl.workflow\n",
    "def create_eval_dataset(\n",
    "    documents: list[CustomDocument],\n",
    "    n_questions_per_doc: int = 1,\n",
    "    n_answers_per_question: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    qa_dataset = create_qa_dataset(\n",
    "        documents=documents,\n",
    "        n_questions_per_doc=n_questions_per_doc,\n",
    "        n_answers_per_question=n_answers_per_question,\n",
    "    )\n",
    "    return create_llm_filtered_dataset(dataset=qa_dataset)\n",
    "\n",
    "\n",
    "knowledge_base_lp = fl.LaunchPlan.get_or_create(\n",
    "    knowledge_base_workflow,\n",
    "    name=\"knowledge_base_lp\",\n",
    "    default_inputs={\"limit\": 10},\n",
    "    schedule=fl.FixedRate(duration=timedelta(minutes=3))\n",
    ")\n",
    "\n",
    "create_eval_dataset_lp = fl.LaunchPlan.get_or_create(\n",
    "    create_eval_dataset,\n",
    "    name=\"create_eval_dataset_lp\",\n",
    "    trigger=OnArtifact(\n",
    "        trigger_on=KnowledgeBase,\n",
    "        inputs={\"documents\": KnowledgeBase.query()},\n",
    "    )\n",
    ")\n",
    "\n",
    "optimize_rag_lp = fl.LaunchPlan.get_or_create(\n",
    "    optimize_rag,\n",
    "    name=\"optimize_rag_lp\",\n",
    "    default_inputs={\n",
    "        \"gridsearch_config\": GridSearchConfig(\n",
    "            embedding_model=[\n",
    "                \"text-embedding-ada-002\",\n",
    "                \"text-embedding-3-small\",\n",
    "                \"text-embedding-3-large\",\n",
    "            ],\n",
    "            chunk_size=[256],\n",
    "            splitter=[\"recursive\"],\n",
    "        ),\n",
    "    },\n",
    "    trigger=OnArtifact(\n",
    "        trigger_on=EvalDatasetArtifact,\n",
    "        inputs={\"eval_dataset\": EvalDatasetArtifact.query()},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register and activate the reactive pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union register reactive_pipeline.py\n",
    "!union launchplan knowledge_base_lp --activate\n",
    "!union launchplan create_eval_dataset_lp --activate\n",
    "!union launchplan optimize_rag_lp --activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deactivate the reactive pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union launchplan knowledge_base_lp --deactivate\n",
    "!union launchplan create_eval_dataset_lp --deactivate\n",
    "!union launchplan optimize_rag_lp --deactivate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
